{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import fastText\n",
    "\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train.csv\", index_col=0)\n",
    "test = pd.read_csv(\"data/test.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['comment_text'] = train['comment_text'].astype(str)\n",
    "test['comment_text'] = test['comment_text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_skipgram_emb = fastText.load_model('data/wiki.en.bin')\n",
    "EMBEDDING_DIM = len(fasttext_skipgram_emb.get_word_vector('test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample():\n",
    "    train_set = train.sample(frac=0.8)\n",
    "    val_set = train.drop(train_set.index)\n",
    "    \n",
    "    class_weight = {0: len(train) / 2 / (len(train) - sum(train['toxic'])), 1: len(train) / 2 / sum(train['toxic']) }\n",
    "    \n",
    "    train_set = train_set.reset_index(drop=True)\n",
    "    val_set = val_set.reset_index(drop=True)\n",
    "    \n",
    "    print(train_set['toxic'].describe())\n",
    "    print(val_set['toxic'].describe())\n",
    "    \n",
    "    return train_set, val_set, class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    127657.000000\n",
      "mean          0.095772\n",
      "std           0.294280\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           0.000000\n",
      "75%           0.000000\n",
      "max           1.000000\n",
      "Name: toxic, dtype: float64\n",
      "count    31914.000000\n",
      "mean         0.096133\n",
      "std          0.294779\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          0.000000\n",
      "max          1.000000\n",
      "Name: toxic, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "train_set, val_set, class_weight = sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "maxlen = 256\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(train['comment_text'].append(test['comment_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = sequence.pad_sequences(tokenizer.texts_to_sequences(train_set['comment_text']), maxlen=maxlen)\n",
    "X_va = sequence.pad_sequences(tokenizer.texts_to_sequences(val_set['comment_text']), maxlen=maxlen)\n",
    "X_te = sequence.pad_sequences(tokenizer.texts_to_sequences(test['comment_text']), maxlen=maxlen)\n",
    "\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "y_tr = train_set[list_classes]\n",
    "y_va = val_set[list_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_matrix[i] = fasttext_skipgram_emb.get_word_vector(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def roc_auc_score(y_true, y_pred):\n",
    "    \"\"\" ROC AUC Score.\n",
    "    Approximates the Area Under Curve score, using approximation based on\n",
    "    the Wilcoxon-Mann-Whitney U statistic.\n",
    "    Yan, L., Dodier, R., Mozer, M. C., & Wolniewicz, R. (2003).\n",
    "    Optimizing Classifier Performance via an Approximation to the Wilcoxon-Mann-Whitney Statistic.\n",
    "    Measures overall performance for a full range of threshold levels.\n",
    "    Arguments:\n",
    "        y_pred: `Tensor`. Predicted values.\n",
    "        y_true: `Tensor` . Targets (labels), a probability distribution.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"RocAucScore\"):\n",
    "\n",
    "        pos = tf.boolean_mask(y_pred, tf.cast(y_true, tf.bool))\n",
    "        neg = tf.boolean_mask(y_pred, ~tf.cast(y_true, tf.bool))\n",
    "\n",
    "        pos = tf.expand_dims(pos, 0)\n",
    "        neg = tf.expand_dims(neg, 1)\n",
    "\n",
    "        # original paper suggests performance is robust to exact parameter choice\n",
    "        gamma = 0.2\n",
    "        p     = 3\n",
    "\n",
    "        difference = tf.zeros_like(pos * neg) + pos - neg - gamma\n",
    "\n",
    "        masked = tf.boolean_mask(difference, difference < 0.0)\n",
    "\n",
    "        return tf.reduce_sum(tf.pow(-masked, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hoiy927/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1247: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/hoiy927/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1213: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/hoiy927/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 256, 300)          118436400 \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 256, 256)          329472    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 256, 256)          295680    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 119,128,886\n",
      "Trainable params: 692,486\n",
      "Non-trainable params: 118,436,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input, Flatten, Activation\n",
    "from keras.layers import Add,Conv1D, MaxPooling1D, Average, Lambda, RepeatVector, LSTM, Bidirectional, GlobalMaxPool1D, Dropout, GRU, Conv1D, Reshape, MaxPooling1D, Concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "from keras.constraints import non_neg, unit_norm\n",
    "from sru import SRU\n",
    "\n",
    "DROPOUT=0.5\n",
    "\n",
    "def mixing_layer(pred, emb):\n",
    "    x = RepeatVector(len(list_classes))(pred)\n",
    "    x = Lambda(lambda x: x * (np.ones([len(list_classes), len(list_classes)]) - np.eye(len(list_classes))))(x)\n",
    "    c = Concatenate()([emb, x])\n",
    "    return c\n",
    "\n",
    "\n",
    "def split_dense(emb_layer, activation):\n",
    "    k = []\n",
    "    for i in range(len(list_classes)):\n",
    "        k.append(Lambda(lambda x: x[:,i,:])(emb_layer))\n",
    "    k = [Dense(1, activation=activation)(s) for s in k]\n",
    "    return Concatenate()(k)    \n",
    "\n",
    "def DPCNN(last_layer):\n",
    "    conv = Conv1D(EMBEDDING_DIM, 3, padding='same')(last_layer)\n",
    "    conv = Dropout(DROPOUT)(conv)\n",
    "    #conv = Conv1D(EMBEDDING_DIM, 3, padding='same')(conv)\n",
    "    #conv = Dropout(DROPOUT)(conv)\n",
    "    conv = Add()([conv, last_layer])\n",
    "    conv = MaxPooling1D(2)(conv)\n",
    "    return Activation('selu')(conv)\n",
    "\n",
    "FREE_EMB_SIZE=8\n",
    "\n",
    "def get_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    \n",
    "    emb = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1], weights=[embedding_matrix], trainable=False)(inp)\n",
    "    emb = Bidirectional(GRU(128, recurrent_dropout=DROPOUT, dropout=DROPOUT, return_sequences=True))(emb)\n",
    "    emb = Bidirectional(GRU(128, recurrent_dropout=DROPOUT, dropout=DROPOUT, return_sequences=True))(emb)\n",
    "    emb = GlobalMaxPool1D()(emb)\n",
    "    \n",
    "    pred = Dense(256, activation='selu')(emb)\n",
    "    pred = Dropout(DROPOUT)(pred)\n",
    "    final = Dense(6, activation='sigmoid')(pred)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=final)\n",
    "    model.compile(loss=roc_auc_score,\n",
    "                  optimizer='adam')\n",
    "\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 4247.2301 - acc: 0.7079 - binary_crossentropy: 1.2831roc-auc_val: 0.9668                                                                                                    \n",
      "127657/127657 [==============================] - 874s 7ms/step - loss: 4227.2671 - acc: 0.7084 - binary_crossentropy: 1.2821 - val_loss: 978.2453 - val_acc: 0.9934 - val_binary_crossentropy: 1.0808\n",
      "Epoch 2/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 1583.6596 - acc: 0.7953 - binary_crossentropy: 1.1508roc-auc_val: 0.9707                                                                                                    \n",
      "127657/127657 [==============================] - 872s 7ms/step - loss: 1577.3108 - acc: 0.7946 - binary_crossentropy: 1.1503 - val_loss: 902.3205 - val_acc: 0.9926 - val_binary_crossentropy: 1.2052\n",
      "Epoch 3/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 1294.7014 - acc: 0.8282 - binary_crossentropy: 1.0652roc-auc_val: 0.9724                                                                                                    \n",
      "127657/127657 [==============================] - 883s 7ms/step - loss: 1293.2200 - acc: 0.8273 - binary_crossentropy: 1.0653 - val_loss: 863.8687 - val_acc: 0.9906 - val_binary_crossentropy: 1.1857\n",
      "Epoch 4/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 1154.2455 - acc: 0.8326 - binary_crossentropy: 1.0345roc-auc_val: 0.9743                                                                                                    \n",
      "127657/127657 [==============================] - 887s 7ms/step - loss: 1151.5493 - acc: 0.8327 - binary_crossentropy: 1.0348 - val_loss: 796.1125 - val_acc: 0.9939 - val_binary_crossentropy: 1.1930\n",
      "Epoch 5/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 1045.1390 - acc: 0.8627 - binary_crossentropy: 1.0092roc-auc_val: 0.9757                                                                                                    \n",
      "127657/127657 [==============================] - 886s 7ms/step - loss: 1042.3211 - acc: 0.8625 - binary_crossentropy: 1.0090 - val_loss: 673.7145 - val_acc: 0.9935 - val_binary_crossentropy: 1.0244\n",
      "Epoch 6/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 951.2605 - acc: 0.8626 - binary_crossentropy: 0.9609roc-auc_val: 0.9776                                                                                                    \n",
      "127657/127657 [==============================] - 887s 7ms/step - loss: 947.6868 - acc: 0.8626 - binary_crossentropy: 0.9607 - val_loss: 645.8069 - val_acc: 0.9921 - val_binary_crossentropy: 1.0214\n",
      "Epoch 7/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 900.2699 - acc: 0.8855 - binary_crossentropy: 0.9478roc-auc_val: 0.9797                                                                                                    \n",
      "127657/127657 [==============================] - 884s 7ms/step - loss: 896.8987 - acc: 0.8855 - binary_crossentropy: 0.9478 - val_loss: 616.8556 - val_acc: 0.9902 - val_binary_crossentropy: 1.0518\n",
      "Epoch 8/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 831.8561 - acc: 0.8955 - binary_crossentropy: 0.9303roc-auc_val: 0.9799                                                                                                    \n",
      "127657/127657 [==============================] - 877s 7ms/step - loss: 828.7860 - acc: 0.8957 - binary_crossentropy: 0.9304 - val_loss: 614.7381 - val_acc: 0.9937 - val_binary_crossentropy: 1.0318\n",
      "Epoch 9/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 774.8325 - acc: 0.8859 - binary_crossentropy: 0.9273roc-auc_val: 0.9817                                                                                                    \n",
      "127657/127657 [==============================] - 885s 7ms/step - loss: 772.0716 - acc: 0.8858 - binary_crossentropy: 0.9269 - val_loss: 553.5548 - val_acc: 0.9892 - val_binary_crossentropy: 0.9531\n",
      "Epoch 10/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 738.5063 - acc: 0.9034 - binary_crossentropy: 0.8997roc-auc_val: 0.9827                                                                                                    \n",
      "127657/127657 [==============================] - 878s 7ms/step - loss: 736.5945 - acc: 0.9029 - binary_crossentropy: 0.8997 - val_loss: 537.4135 - val_acc: 0.9840 - val_binary_crossentropy: 0.9922\n",
      "Epoch 11/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 703.1700 - acc: 0.9076 - binary_crossentropy: 0.8907roc-auc_val: 0.9807                                                                                                    \n",
      "127657/127657 [==============================] - 881s 7ms/step - loss: 701.0789 - acc: 0.9077 - binary_crossentropy: 0.8913 - val_loss: 609.4034 - val_acc: 0.9915 - val_binary_crossentropy: 1.0784\n",
      "Epoch 12/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 664.6432 - acc: 0.9068 - binary_crossentropy: 0.8624roc-auc_val: 0.9832                                                                                                    \n",
      "127657/127657 [==============================] - 887s 7ms/step - loss: 662.5538 - acc: 0.9064 - binary_crossentropy: 0.8623 - val_loss: 521.4866 - val_acc: 0.9752 - val_binary_crossentropy: 0.9422\n",
      "Epoch 13/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 645.6296 - acc: 0.9105 - binary_crossentropy: 0.8615roc-auc_val: 0.9841                                                                                                    \n",
      "127657/127657 [==============================] - 871s 7ms/step - loss: 643.2406 - acc: 0.9106 - binary_crossentropy: 0.8613 - val_loss: 501.6516 - val_acc: 0.9853 - val_binary_crossentropy: 0.9115\n",
      "Epoch 14/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 625.2839 - acc: 0.9100 - binary_crossentropy: 0.8363roc-auc_val: 0.9838                                                                                                    \n",
      "127657/127657 [==============================] - 883s 7ms/step - loss: 622.7585 - acc: 0.9101 - binary_crossentropy: 0.8362 - val_loss: 492.1870 - val_acc: 0.9838 - val_binary_crossentropy: 0.9103\n",
      "Epoch 15/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 604.6645 - acc: 0.9144 - binary_crossentropy: 0.8375roc-auc_val: 0.9846                                                                                                    \n",
      "127657/127657 [==============================] - 889s 7ms/step - loss: 602.7729 - acc: 0.9144 - binary_crossentropy: 0.8374 - val_loss: 462.6437 - val_acc: 0.9851 - val_binary_crossentropy: 0.8792\n",
      "Epoch 16/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 574.3125 - acc: 0.9200 - binary_crossentropy: 0.8202roc-auc_val: 0.9837                                                                                                    \n",
      "127657/127657 [==============================] - 881s 7ms/step - loss: 572.2399 - acc: 0.9198 - binary_crossentropy: 0.8202 - val_loss: 485.5067 - val_acc: 0.9883 - val_binary_crossentropy: 0.9056\n",
      "Epoch 17/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 561.3721 - acc: 0.9264 - binary_crossentropy: 0.8124roc-auc_val: 0.9842                                                                                                    \n",
      "127657/127657 [==============================] - 878s 7ms/step - loss: 559.9199 - acc: 0.9262 - binary_crossentropy: 0.8123 - val_loss: 467.4715 - val_acc: 0.9776 - val_binary_crossentropy: 0.8710\n",
      "Epoch 18/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 546.2786 - acc: 0.8978 - binary_crossentropy: 0.8095roc-auc_val: 0.9851                                                                                                    \n",
      "127657/127657 [==============================] - 881s 7ms/step - loss: 544.3287 - acc: 0.8979 - binary_crossentropy: 0.8096 - val_loss: 451.1144 - val_acc: 0.9816 - val_binary_crossentropy: 0.8759\n",
      "Epoch 19/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126976/127657 [============================>.] - ETA: 3s - loss: 521.9551 - acc: 0.9194 - binary_crossentropy: 0.7952roc-auc_val: 0.985                                                                                                    \n",
      "127657/127657 [==============================] - 887s 7ms/step - loss: 520.4238 - acc: 0.9194 - binary_crossentropy: 0.7953 - val_loss: 445.1148 - val_acc: 0.9780 - val_binary_crossentropy: 0.8598\n",
      "Epoch 20/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 512.9752 - acc: 0.9143 - binary_crossentropy: 0.7841roc-auc_val: 0.9857                                                                                                    \n",
      "127657/127657 [==============================] - 890s 7ms/step - loss: 511.3506 - acc: 0.9144 - binary_crossentropy: 0.7842 - val_loss: 424.5755 - val_acc: 0.9852 - val_binary_crossentropy: 0.8515\n",
      "Epoch 21/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 495.6329 - acc: 0.9110 - binary_crossentropy: 0.7791roc-auc_val: 0.9859                                                                                                    \n",
      "127657/127657 [==============================] - 885s 7ms/step - loss: 494.5459 - acc: 0.9111 - binary_crossentropy: 0.7790 - val_loss: 414.5322 - val_acc: 0.9842 - val_binary_crossentropy: 0.8201\n",
      "Epoch 22/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 489.8173 - acc: 0.9202 - binary_crossentropy: 0.7730roc-auc_val: 0.9857                                                                                                    \n",
      "127657/127657 [==============================] - 876s 7ms/step - loss: 488.1795 - acc: 0.9202 - binary_crossentropy: 0.7730 - val_loss: 424.4533 - val_acc: 0.9755 - val_binary_crossentropy: 0.8118\n",
      "Epoch 23/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 468.9828 - acc: 0.9331 - binary_crossentropy: 0.7606roc-auc_val: 0.9858                                                                                                    \n",
      "127657/127657 [==============================] - 887s 7ms/step - loss: 467.2733 - acc: 0.9331 - binary_crossentropy: 0.7606 - val_loss: 412.5062 - val_acc: 0.9814 - val_binary_crossentropy: 0.8240\n",
      "Epoch 24/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 466.1705 - acc: 0.9344 - binary_crossentropy: 0.7670roc-auc_val: 0.9857                                                                                                    \n",
      "127657/127657 [==============================] - 868s 7ms/step - loss: 464.9374 - acc: 0.9345 - binary_crossentropy: 0.7669 - val_loss: 418.2035 - val_acc: 0.9777 - val_binary_crossentropy: 0.7747\n",
      "Epoch 25/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 451.6446 - acc: 0.9453 - binary_crossentropy: 0.7438roc-auc_val: 0.986                                                                                                    \n",
      "127657/127657 [==============================] - 881s 7ms/step - loss: 450.2888 - acc: 0.9452 - binary_crossentropy: 0.7439 - val_loss: 406.0258 - val_acc: 0.9793 - val_binary_crossentropy: 0.7931\n",
      "Epoch 26/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 451.0385 - acc: 0.9309 - binary_crossentropy: 0.7420roc-auc_val: 0.986                                                                                                    \n",
      "127657/127657 [==============================] - 883s 7ms/step - loss: 449.7029 - acc: 0.9311 - binary_crossentropy: 0.7421 - val_loss: 411.7446 - val_acc: 0.9833 - val_binary_crossentropy: 0.7850\n",
      "Epoch 27/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 436.4091 - acc: 0.9438 - binary_crossentropy: 0.7330roc-auc_val: 0.9861                                                                                                    \n",
      "127657/127657 [==============================] - 880s 7ms/step - loss: 435.1955 - acc: 0.9438 - binary_crossentropy: 0.7332 - val_loss: 399.5614 - val_acc: 0.9811 - val_binary_crossentropy: 0.8009\n",
      "Epoch 28/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 435.4693 - acc: 0.9426 - binary_crossentropy: 0.7275roc-auc_val: 0.9861                                                                                                    \n",
      "127657/127657 [==============================] - 883s 7ms/step - loss: 434.2588 - acc: 0.9425 - binary_crossentropy: 0.7275 - val_loss: 393.4555 - val_acc: 0.9746 - val_binary_crossentropy: 0.7745\n",
      "Epoch 29/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 421.2111 - acc: 0.9340 - binary_crossentropy: 0.7319roc-auc_val: 0.9864                                                                                                    \n",
      "127657/127657 [==============================] - 881s 7ms/step - loss: 420.0041 - acc: 0.9340 - binary_crossentropy: 0.7318 - val_loss: 398.3785 - val_acc: 0.9780 - val_binary_crossentropy: 0.7391\n",
      "Epoch 30/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 415.5127 - acc: 0.9397 - binary_crossentropy: 0.7171roc-auc_val: 0.9869                                                                                                    \n",
      "127657/127657 [==============================] - 889s 7ms/step - loss: 414.1504 - acc: 0.9397 - binary_crossentropy: 0.7171 - val_loss: 385.5721 - val_acc: 0.9758 - val_binary_crossentropy: 0.7370\n",
      "Epoch 31/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 404.9849 - acc: 0.9481 - binary_crossentropy: 0.7115roc-auc_val: 0.9872                                                                                                    \n",
      "127657/127657 [==============================] - 873s 7ms/step - loss: 403.5766 - acc: 0.9482 - binary_crossentropy: 0.7115 - val_loss: 376.6767 - val_acc: 0.9814 - val_binary_crossentropy: 0.7307\n",
      "Epoch 32/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 400.6407 - acc: 0.9359 - binary_crossentropy: 0.7000roc-auc_val: 0.9864                                                                                                    \n",
      "127657/127657 [==============================] - 879s 7ms/step - loss: 399.1619 - acc: 0.9358 - binary_crossentropy: 0.6999 - val_loss: 384.6698 - val_acc: 0.9782 - val_binary_crossentropy: 0.7072\n",
      "Epoch 33/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 387.5761 - acc: 0.9218 - binary_crossentropy: 0.6966roc-auc_val: 0.9873                                                                                                    \n",
      "127657/127657 [==============================] - 877s 7ms/step - loss: 386.5413 - acc: 0.9221 - binary_crossentropy: 0.6967 - val_loss: 376.9243 - val_acc: 0.9846 - val_binary_crossentropy: 0.7245\n",
      "Epoch 34/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 382.5659 - acc: 0.9441 - binary_crossentropy: 0.6784roc-auc_val: 0.9871                                                                                                    \n",
      "127657/127657 [==============================] - 887s 7ms/step - loss: 380.9789 - acc: 0.9442 - binary_crossentropy: 0.6786 - val_loss: 371.9848 - val_acc: 0.9829 - val_binary_crossentropy: 0.7371\n",
      "Epoch 35/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 376.6725 - acc: 0.9477 - binary_crossentropy: 0.6750roc-auc_val: 0.9873                                                                                                    \n",
      "127657/127657 [==============================] - 887s 7ms/step - loss: 375.3401 - acc: 0.9478 - binary_crossentropy: 0.6750 - val_loss: 365.9431 - val_acc: 0.9809 - val_binary_crossentropy: 0.6863\n",
      "Epoch 36/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 371.2070 - acc: 0.9471 - binary_crossentropy: 0.6678roc-auc_val: 0.9872                                                                                                    \n",
      "127657/127657 [==============================] - 880s 7ms/step - loss: 369.9739 - acc: 0.9470 - binary_crossentropy: 0.6679 - val_loss: 372.7471 - val_acc: 0.9747 - val_binary_crossentropy: 0.7054\n",
      "Epoch 37/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 363.1325 - acc: 0.9399 - binary_crossentropy: 0.6631roc-auc_val: 0.9873                                                                                                    \n",
      "127657/127657 [==============================] - 879s 7ms/step - loss: 361.9699 - acc: 0.9398 - binary_crossentropy: 0.6632 - val_loss: 363.2920 - val_acc: 0.9761 - val_binary_crossentropy: 0.6986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 358.9288 - acc: 0.9350 - binary_crossentropy: 0.6594roc-auc_val: 0.9871                                                                                                    \n",
      "127657/127657 [==============================] - 866s 7ms/step - loss: 357.9502 - acc: 0.9350 - binary_crossentropy: 0.6592 - val_loss: 366.4690 - val_acc: 0.9817 - val_binary_crossentropy: 0.6586\n",
      "Epoch 39/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 351.7428 - acc: 0.9489 - binary_crossentropy: 0.6475roc-auc_val: 0.9869                                                                                                    \n",
      "127657/127657 [==============================] - 873s 7ms/step - loss: 350.9053 - acc: 0.9489 - binary_crossentropy: 0.6477 - val_loss: 364.6344 - val_acc: 0.9786 - val_binary_crossentropy: 0.6711\n",
      "Epoch 40/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 348.1070 - acc: 0.9492 - binary_crossentropy: 0.6428roc-auc_val: 0.9874                                                                                                    \n",
      "127657/127657 [==============================] - 865s 7ms/step - loss: 346.9242 - acc: 0.9494 - binary_crossentropy: 0.6429 - val_loss: 364.1575 - val_acc: 0.9859 - val_binary_crossentropy: 0.6647\n",
      "Epoch 41/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 350.6856 - acc: 0.9476 - binary_crossentropy: 0.6395roc-auc_val: 0.9878                                                                                                    \n",
      "127657/127657 [==============================] - 884s 7ms/step - loss: 349.3636 - acc: 0.9478 - binary_crossentropy: 0.6396 - val_loss: 350.9706 - val_acc: 0.9819 - val_binary_crossentropy: 0.6605\n",
      "Epoch 42/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 342.4894 - acc: 0.9478 - binary_crossentropy: 0.6389roc-auc_val: 0.9879                                                                                                    \n",
      "127657/127657 [==============================] - 877s 7ms/step - loss: 341.6427 - acc: 0.9479 - binary_crossentropy: 0.6389 - val_loss: 351.6418 - val_acc: 0.9809 - val_binary_crossentropy: 0.6578\n",
      "Epoch 43/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 341.7124 - acc: 0.9490 - binary_crossentropy: 0.6294roc-auc_val: 0.9883                                                                                                    \n",
      "127657/127657 [==============================] - 879s 7ms/step - loss: 340.7035 - acc: 0.9491 - binary_crossentropy: 0.6295 - val_loss: 348.5475 - val_acc: 0.9742 - val_binary_crossentropy: 0.6490\n",
      "Epoch 44/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 328.7671 - acc: 0.9453 - binary_crossentropy: 0.6314roc-auc_val: 0.9884                                                                                                    \n",
      "127657/127657 [==============================] - 881s 7ms/step - loss: 327.7779 - acc: 0.9453 - binary_crossentropy: 0.6313 - val_loss: 347.2139 - val_acc: 0.9602 - val_binary_crossentropy: 0.6231\n",
      "Epoch 45/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 324.2311 - acc: 0.9325 - binary_crossentropy: 0.6235roc-auc_val: 0.9882                                                                                                    \n",
      "127657/127657 [==============================] - 870s 7ms/step - loss: 323.5258 - acc: 0.9325 - binary_crossentropy: 0.6234 - val_loss: 346.2816 - val_acc: 0.9673 - val_binary_crossentropy: 0.6296\n",
      "Epoch 46/2000\n",
      "126976/127657 [============================>.] - ETA: 3s - loss: 317.5903 - acc: 0.9301 - binary_crossentropy: 0.6143roc-auc_val: 0.9884                                                                                                    \n",
      "127657/127657 [==============================] - 879s 7ms/step - loss: 316.8418 - acc: 0.9301 - binary_crossentropy: 0.6144 - val_loss: 336.4793 - val_acc: 0.9717 - val_binary_crossentropy: 0.6351\n",
      "Epoch 47/2000\n",
      " 40960/127657 [========>.....................] - ETA: 8:14 - loss: 322.0985 - acc: 0.9530 - binary_crossentropy: 0.6228"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import Callback\n",
    "from sklearn import metrics\n",
    "\n",
    "np.random.seed(777)\n",
    "batch_size = 1024\n",
    "epochs = 2000\n",
    "\n",
    "class roc_callback(Callback):\n",
    "    def __init__(self,training_data,validation_data):\n",
    "        \n",
    "        self.x = training_data[0]\n",
    "        self.y = training_data[1]\n",
    "        self.x_val = validation_data[0]\n",
    "        self.y_val = validation_data[1]\n",
    "        \n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    " \n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    " \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    " \n",
    "    def on_epoch_end(self, epoch, logs={}):        \n",
    "        #y_pred = self.model.predict(self.x, batch_size=batch_size)\n",
    "        #roc = metrics.roc_auc_score(self.y, y_pred)\n",
    "        \n",
    "        y_pred_val = self.model.predict(self.x_val, batch_size=batch_size)\n",
    "        roc_val = metrics.roc_auc_score(self.y_val, y_pred_val)\n",
    "        \n",
    "        print('roc-auc_val: %s' % str(round(roc_val,4)),end=100*' '+'\\n')\n",
    "        \n",
    "        #print('\\rroc-auc: %s - roc-auc_val: %s' % (str(round(roc,4)),str(round(roc_val,4))),end=100*' '+'\\n')\n",
    "        return\n",
    " \n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    " \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return   \n",
    "\n",
    "file_path=\"weights.{epoch:02d}-{val_loss:.2f}..hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', period=3)\n",
    "#early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "callbacks_list = [checkpoint, roc_callback((X_tr, y_tr), (X_va, y_va))]\n",
    "#callbacks_list = [checkpoint, early]\n",
    "model.fit(\n",
    "    X_tr,\n",
    "    y_tr, \n",
    "    class_weight=None, \n",
    "    validation_data=(X_va, y_va), \n",
    "    shuffle=True, \n",
    "    batch_size=batch_size, \n",
    "    epochs=epochs, \n",
    "    callbacks=callbacks_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(file_path)\n",
    "pred = model.predict(X_va, batch_size=128)\n",
    "int_pred = pred >= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "for i, c in enumerate(list_classes):\n",
    "    print(c)\n",
    "    print(\"roc:\\t\\t%.5f\" % metrics.roc_auc_score(val_set[c], pred[:,i]))\n",
    "    print(\"f1:\\t\\t%.5f\" % metrics.f1_score(val_set[c], int_pred[:,i]))\n",
    "    print(\"precision:\\t%.5f\" % metrics.precision_score(val_set[c], int_pred[:,i]))\n",
    "    print(\"recall:\\t\\t%.5f\" %metrics.recall_score(val_set[c], int_pred[:,i]))\n",
    "    print(\"log loss:\\t%.f\" %metrics.log_loss(val_set[c], pred[:,i]))\n",
    "\n",
    "    m = metrics.confusion_matrix(val_set[c], int_pred[:,i])\n",
    "    tp = m[1,1]\n",
    "    fp = m[0,1]\n",
    "    tn = m[0,0]\n",
    "    fn = m[1,0]\n",
    "    print(\"tp:\\t\\t%d\"%tp)\n",
    "    print(\"fp:\\t\\t%d\"%fp)\n",
    "    print(\"tn:\\t\\t%d\"%tn)\n",
    "    print(\"fn:\\t\\t%d\"%fn)\n",
    "    \n",
    "    print(\"tpr:\\t\\t%.3f\"%(tp / (tp+fn)))\n",
    "    print(\"fpr:\\t\\t%.3f\"%(fp / (fp+tn)))\n",
    "\n",
    "    \n",
    "    precision, recall, threshold = metrics.precision_recall_curve(val_set[c], pred[:,i])\n",
    "    plt.figure(0)\n",
    "    plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "    plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "         color='b')\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('2-class Precision-Recall curve:')\n",
    "    plt.show()\n",
    "    \n",
    "    fpr, tpr, _ = metrics.roc_curve(val_set[c], pred[:,i])\n",
    "    plt.figure(1)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in val_set[(pred[:, 0] <= 0.1) & (val_set['toxic'] == 1)].sample(1).iterrows():\n",
    "    print(row['comment_text'])\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(file_path)\n",
    "sample_submission = pd.read_csv(\"./data/sample_submission.csv\")\n",
    "\n",
    "sample_submission[list_classes] = y_test\n",
    "\n",
    "sample_submission.to_csv(\"baseline.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(file_path)\n",
    "\n",
    "y_test = model.predict([X_te], batch_size=2048)\n",
    "\n",
    "sample_submission = pd.read_csv(\"./data/sample_submission.csv\")\n",
    "sample_submission[list_classes] = y_test\n",
    "sample_submission.to_csv(\"baseline.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_counts['pagan']\n",
    "tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
