{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train.csv\", index_col=0)\n",
    "test = pd.read_csv(\"data/test.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample2():\n",
    "    train_set = train.sample(frac=0.8)\n",
    "    val_set = train.drop(train_set.index)\n",
    "    \n",
    "    class_weight = {0: len(train) / 2 / (len(train) - sum(train['toxic'])), 1: len(train) / 2 / sum(train['toxic']) }\n",
    "    \n",
    "    train_set = train_set.reset_index(drop=True)\n",
    "    val_set = val_set.reset_index(drop=True)\n",
    "    \n",
    "    print(train_set['toxic'].describe())\n",
    "    print(val_set['toxic'].describe())\n",
    "    \n",
    "    return train_set, val_set, class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    127657.000000\n",
      "mean          0.096133\n",
      "std           0.294774\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           0.000000\n",
      "75%           0.000000\n",
      "max           1.000000\n",
      "Name: toxic, dtype: float64\n",
      "count    31914.000000\n",
      "mean         0.094692\n",
      "std          0.292794\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          0.000000\n",
      "max          1.000000\n",
      "Name: toxic, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "train_set, val_set, class_weight = sample2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 256\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(train_set['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_word_index = { v:k for k, v in tokenizer.word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = sequence.pad_sequences(tokenizer.texts_to_sequences(train_set['comment_text']), maxlen=maxlen)\n",
    "X_va = sequence.pad_sequences(tokenizer.texts_to_sequences(val_set['comment_text']), maxlen=maxlen)\n",
    "X_te = sequence.pad_sequences(tokenizer.texts_to_sequences(test['comment_text']), maxlen=maxlen)\n",
    "\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "other = [\"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "y_tr = train_set[list_classes]\n",
    "y_va = val_set[list_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 256, 384)     7680000     input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 384)          0           embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 384)          0           global_max_pooling1d_8[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 384)          0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, 6, 64)        0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 6, 1)         65          reshape_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 6)            0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_7 (RepeatVector)  (None, 6, 6)         0           flatten_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 6, 6)         0           repeat_vector_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 6, 70)        0           reshape_7[0][0]                  \n",
      "                                                                 lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 6, 1)         71          concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 6)            0           dense_14[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 7,680,136\n",
      "Trainable params: 7,680,136\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input, Flatten, Activation\n",
    "from keras.layers import Lambda, RepeatVector, LSTM, Bidirectional, GlobalMaxPool1D, Dropout, GRU, Conv1D, Reshape, MaxPooling1D, Concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "from keras.constraints import non_neg, unit_norm\n",
    "\n",
    "DROPOUT=0.1\n",
    "\n",
    "def encoder(inp):\n",
    "    \n",
    "    glove2 = Embedding(max_features, 2048)(inp)\n",
    "    enc2 = GlobalMaxPool1D()(glove2)\n",
    "\n",
    "    \"\"\"\n",
    "    glove = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False)(inp)\n",
    "    enc = Bidirectional(GRU(LSTM_SIZE, dropout=0.1, recurrent_dropout=0.1, activation='selu'))(glove)\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    enc = Conv1D(CONV_SIZE * 2, 2, strides=1, padding='valid', activation='relu')(glove)\n",
    "    enc = MaxPooling1D(4)(enc)\n",
    "    enc = Conv1D(CONV_SIZE * 4, 2, strides=1, padding='valid', activation='relu')(enc)\n",
    "    enc = GlobalMaxPool1D()(enc)\n",
    "    \n",
    "    enc = Flatten()(enc)\n",
    "    \"\"\"\n",
    "    #enc = Concatenate()([enc2, enc])\n",
    "    return enc2\n",
    "\n",
    "\n",
    "def mixing_layer(pred, max_pool):\n",
    "    x = RepeatVector(len(list_classes))(pred)\n",
    "    x = Lambda(lambda x: x * (np.ones([len(list_classes), len(list_classes)]) - np.eye(len(list_classes))))(x)\n",
    "    c = Concatenate()([max_pool, x])\n",
    "    return c\n",
    "\n",
    "def get_model_mix_layer():\n",
    "    embed_size = len(list_classes) * 64\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Activation('selu')(x)\n",
    "    \n",
    "    x = Dropout(DROPOUT)(x)    \n",
    "    max_pool = Reshape([len(list_classes), -1])(x)\n",
    "\n",
    "    \n",
    "    pred = Dense(1, activation=\"sigmoid\")(max_pool)\n",
    "    pred = Flatten()(pred)\n",
    "    \n",
    "    mix = mixing_layer(pred, max_pool)\n",
    "    \n",
    "    x = Dense(1, activation='sigmoid')(mix)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy', 'binary_crossentropy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model_orig():\n",
    "    embed_size = 32\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    #other_inp = Input(shape=(len(other), ))\n",
    "    feat_inp = Input(shape=(len(manual_features), ))\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    \"\"\"\n",
    "    enc = encoder(inp)\n",
    "\n",
    "    \"\"\"\n",
    "    g = Bidirectional(GRU(LSTM_SIZE, return_sequences=True))(g)\n",
    "    g = Bidirectional(GRU(LSTM_SIZE))(g)\n",
    "\n",
    "    g = Conv1D(CONV_SIZE, 2, strides=1, padding='valid', activation='selu')(g)\n",
    "    g = MaxPooling1D(2)(g)\n",
    "    g = Conv1D(CONV_SIZE, 2, strides=1, padding='valid', activation='selu')(g)\n",
    "    g = MaxPooling1D(2)(g)\n",
    "    g = Conv1D(CONV_SIZE, 2, strides=1, padding='valid', activation='selu')(g)\n",
    "    g = MaxPooling1D(2)(g)\n",
    "    g = Conv1D(CONV_SIZE, 2, strides=1, padding='valid', activation='selu')(g)\n",
    "    g = MaxPooling1D(2)(g)    \n",
    "    glove_emb_max = GlobalMaxPool1D()(g)\n",
    "    \"\"\"\n",
    "    \n",
    "    #x = GlobalMaxPool1D()(x)\n",
    "    #others = Dense(10, activation=\"selu\")(other_inp)\n",
    "    \n",
    "    #x = Concatenate()([enc, feat_inp])\n",
    "    x=enc\n",
    "    #x = Dense(100, activation=\"selu\")(x)\n",
    "    \n",
    "    pred = Dense(len(other)+1, activation=\"sigmoid\"\n",
    "                 #bias_constraint=non_neg(), \n",
    "                 #kernel_constraint=non_neg()\n",
    "                )(x)\n",
    "    \n",
    "    model = Model(inputs=[inp, feat_inp], outputs=pred)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy', 'binary_crossentropy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, EMB_SIZE, embeddings_regularizer=l2(1e-3))(inp)\n",
    "    #x = Reshape((maxlen, embed_size, 1))(x)\n",
    "    #x = Bidirectional(LSTM(LSTM_SIZE, return_sequences=True))(x)\n",
    "    x = Conv1D(CONV_SIZE, 2, strides=1, padding='valid', activation='selu')(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = Conv1D(CONV_SIZE, 2, strides=1, padding='valid', activation='selu')(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = Conv1D(CONV_SIZE, 2, strides=1, padding='valid', activation='selu')(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = Conv1D(CONV_SIZE, 2, strides=1, padding='valid', activation='selu')(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = Bidirectional(LSTM(LSTM_SIZE))(x)\n",
    "    #x = Dense(LSTM_SIZE, activation=\"selu\")(x)\n",
    "    x = Dense(len(list_classes), activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy', 'binary_crossentropy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = get_model_mix_layer()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "159571/159571 [==============================] - 239s 1ms/step - loss: 0.0798 - acc: 0.9764 - binary_crossentropy: 0.0798\n",
      "Epoch 2/2\n",
      "159571/159571 [==============================] - 240s 2ms/step - loss: 0.0424 - acc: 0.9837 - binary_crossentropy: 0.0424\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff630e97e48>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "batch_size = 40\n",
    "epochs = 2\n",
    "\n",
    "file_path=\"weights_base.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='loss', save_best_only=True)\n",
    "early = EarlyStopping(monitor=\"loss\", mode=\"min\", patience=5)\n",
    "callbacks_list = [checkpoint, early] #early\n",
    "model.fit(\n",
    "    X_tr, \n",
    "    y_tr, \n",
    "    class_weight=None, \n",
    "    #validation_data=(X_va, y_va), \n",
    "    shuffle=True, \n",
    "    batch_size=batch_size, \n",
    "    epochs=epochs, \n",
    "    callbacks=callbacks_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(file_path)\n",
    "\n",
    "y_test = model.predict(X_te, batch_size=2048)\n",
    "\n",
    "sample_submission = pd.read_csv(\"./data/sample_submission.csv\")\n",
    "sample_submission[['toxic'] + other] = y_test\n",
    "sample_submission.to_csv(\"baseline.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-aa40982cbcc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "y_train = model.predict(X_tr, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
